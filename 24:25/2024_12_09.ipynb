{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ReLUHatSigmoid2024</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51197b47ed584a8299c1aa62d1a83843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w', max=50.0, min=-50.0, step=1.0), FloatSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "@interact\n",
    "def sigmoid(w=widgets.FloatSlider(value=1.0, min=-50.0, max=50.0, step=1),\n",
    "            b=widgets.FloatSlider(value=-0.0, min=-100.0, max=100.0, step=1.0)):\n",
    "    x = np.linspace(-10, 10, 200)\n",
    "    z = x*w + b\n",
    "    y = np.exp(z)/(np.exp(z) + 1)\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.ylabel('Sigmoid(x)')\n",
    "    plt.xlabel('x')\n",
    "    plt.title('Sigmoid function.')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c1d9be440a4389ab6f95031198d97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='s1', max=3.0, min=-3.0), FloatSlider(value=1.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def sigm_comp(s1=widgets.FloatSlider(min=-3.0, max=3.0, step=0.1, value=1.),\n",
    "              s2=widgets.FloatSlider(min=-3.0, max=3.0, step=0.1, value=1.),\n",
    "              wo1=widgets.FloatSlider(min=-3.0, max=3.0, step=0.1, value=1.),\n",
    "            wo2=widgets.FloatSlider(min=-3.0, max=3.0, step=0.1, value=2),\n",
    "            h=widgets.FloatSlider(min=-10.0, max=10.0, step=0.1, value=1)):\n",
    "    x = np.linspace(-2, 2, 400)\n",
    "\n",
    "    w1 = w2 = 500\n",
    "    b1 = -s1 * w1\n",
    "    b2 = -s2 * w2\n",
    "    z1 = x*w1 + b1\n",
    "    y1 = np.exp(z1)/(np.exp(z1) + 1)\n",
    "\n",
    "    z2 = x*w2 + b2\n",
    "    y2 = np.exp(z2)/(np.exp(z2) + 1)\n",
    "\n",
    "    yo = h*(wo1 * y1 + wo2 * y2)\n",
    "\n",
    "    plt.plot(x, yo)\n",
    "    plt.ylabel('Composite Sigmoid(x)')\n",
    "    plt.xlabel('x')\n",
    "    plt.title('Composite Sigmoid function.')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d064eb1cd441cb8854054d6db480fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w', max=10.0, min=-10.0, step=1.0), FloatSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def relu(w=widgets.FloatSlider(value=1.0, min=-10.0, max=10.0, step=1),\n",
    "            b=widgets.FloatSlider(value=-5.0, min=-10.0, max=10.0, step=1.0)):\n",
    "    x = np.linspace(-10, 10, 200)\n",
    "    z = x*w + b\n",
    "    y = np.maximum(0, z)\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.ylabel('ReLu(x)')\n",
    "    plt.xlabel('x')\n",
    "    plt.title('ReLu function.')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573d809ad8fc40ee8121fe86ac26e36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=5.0, description='w1', max=10.0, min=-10.0, step=1.0), FloatSlider(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def relu_comp(w1=widgets.FloatSlider(min=-10.0,max=10.0,step=1.,value=5.),\n",
    "              w2=widgets.FloatSlider(min=-10.0,max=10.0,step=1.,value=4.),\n",
    "              w3=widgets.FloatSlider(min=-10.0,max=10.0,step=1.,value=3.),\n",
    "              b1=widgets.FloatSlider(min=-10.0,max=10.0,step=1.,value=2.),\n",
    "              b2=widgets.FloatSlider(min=-10.0,max=10.0,step=1.,value=1.),\n",
    "              b3=widgets.FloatSlider(min=-10.0,max=10.0,step=1.,value=0.),\n",
    "              wo1=widgets.FloatSlider(min=-3.0,max=3.0,step=1.,value=1.),\n",
    "              wo2=widgets.FloatSlider(min=-3.0,max=3.0,step=1.,value=2),\n",
    "              wo3=widgets.FloatSlider(min=-3.0,max=3.0,step=1.,value=3.),\n",
    "              h=widgets.FloatSlider(min=-1.0,max=1.0,step=0.1,value=1)):\n",
    "    #1,1,1 - 1,0,-1 - 1,-2,1 - 1 Hat\n",
    "    #1,1,0 - 0,-1,0 - 1,-1,0 - 1 Sigmoid\n",
    "    x = np.linspace(-5, 5, 400)\n",
    "\n",
    "    z1 = x*w1 + b1\n",
    "    y1 = np.maximum(0, z1)\n",
    "\n",
    "    z2 = x*w2 + b2\n",
    "    y2 = np.maximum(0, z2)\n",
    "\n",
    "    z3 = x*w3 + b3\n",
    "    y3 = np.maximum(0, z3)\n",
    "\n",
    "    yo = h*(wo1 * y1 + wo2 * y2 + wo3 * y3)\n",
    "\n",
    "    plt.plot(x, yo)\n",
    "    plt.ylabel('Composite ReLu(x)')\n",
    "    plt.xlabel('x')\n",
    "    plt.title('Composite ReLu function.')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sigmoidal functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Definition 1.</strong> A function $\\sigma: \\mathbb{R} \\to [0,1]$ is called a sigmoidal function if\n",
    "$$\n",
    "\\lim_{x \\to -\\infty} \\sigma(x) = 0 \\quad \\text{and} \\quad \\lim_{x \\to +\\infty} \\sigma(x) = 1.\n",
    "$$\n",
    "<strong>Definition 2.</strong> Let $n$ be a natural number and $I_n = [0,1]^n$. We say that an activation function $f: \\mathbb{R} \\to \\mathbb{R}$ is $n$-discriminatory if the only signet Borel measure $\\mu$ such that\n",
    "$$\n",
    "\\int_{I_n} f(x \\cdot w) d\\mu(w) = 0, \\forall y \\in \\mathbb{R}^n, \\theta \\in \\mathbb{R}\n",
    "$$\n",
    "is the zero measure.<br><br>\n",
    "<strong>Definition 3.</strong> We say an activation function $f: \\mathbb{R} \\to \\mathbb{R}$ is discriminatory if it is $n$-discriminatory for any $n$.<br><br>\n",
    "<strong>Remark 1.</strong> A discriminatory function $\\sigma$ is volumetrically non-destructive when it acts on linear transformations of input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Universal approximation theorem</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x$ be the input variable, $z$ the target and denote the target function by $z=f(x)$, with $f$ in a certain function space $S$.<br>\n",
    "Some definitions:\n",
    "- $I_n=[0,1]^n$;\n",
    "- A subspace $U$ of $X$ is <i>dense</i> in $X$ with respect to a norm $\\|\\cdot\\|$ if for every $x \\in X$ there are elements $u \\in U$ as close as possible to $x$. Alternatively:\n",
    "    - $\\forall x \\in X$ there is a sequence $u_n$ in $U$ s.t. $u_n \\to x, as n \\to \\infty$;\n",
    "    - $\\forall x \\in X$, $\\forall \\epsilon > 0$, there is $u \\in U$ s.t. $||u-x|| < \\epsilon$.\n",
    "- The fact that the subspace $U$ is not dense in $X$ can be described as:\n",
    "    - There elements $x_0 \\in X$ s.t. no elements $u \\in U$ are close enough to $x_0$;\n",
    "    - There is a $\\delta > 0$ s.t. $\\forall u \\in U$, we have $||u-x_0|| \\geq \\delta$.\n",
    "- We say that the neural network is a <i>universal approximator</i> for the space $(S, d)$ if the space of outcomes $U$ is $d$-dense in $S$ i.e.\n",
    "$$\n",
    "\\forall f \\in S, \\forall \\epsilon > 0, \\exists u \\in U \\text{ s.t. } d(f,u) < \\epsilon.\n",
    "$$\n",
    "In practice it means that for any funxtion $f \\in S$, there are functions in $U$ situated in any proximity of $f$.\n",
    "- Let $K$ denote a compact set in $\\mathbb{R}^n$ and denote by $C(K)$ the set of real-valued continuous function on $K$;\n",
    "- $M(I_n)$ is the space of finite signed regular Borel measure on $I_n$. (Note: regular means that while different measures may define different sizes for a single set, they all ideally convey some idea of how much space that set takes up relative to the larger space in which it resides).<br><br>\n",
    "<strong>Theorem (representation of linear bounded functional).</strong> Let $F$ be a bounded linear functional on $C(K)$. Then there exists a unique finite sign Borel measure $\\mu$ on $K$ such that\n",
    "$$\n",
    "F(f) = \\int_K f(x) d\\mu(x), \\forall f \\in C(K).\n",
    "$$\n",
    "Moreover $||F||=|\\mu|(K)$.<br><br>\n",
    "<strong>Theorem (Hahn-Banach).</strong>Let $X$ be a linear real vector space, $X_0$ a linear subspace, $p$ a linear convex functional on $X$, and $f:X_0 \\to \\mathbb{R}$ a linear functional s.t. $f(x) \\leq p(x)$ for all $x \\in X_0$. Then there is a linear functional $F: X \\to \\mathbb{R}$ s.t.:\n",
    "- $F_{X_0} = f$ (the restriction of $F$ to $X_0$ is $f$);\n",
    "- $F(x) \\leq p(x)$ for all $x \\in X$;<br><br>\n",
    "\n",
    "<strong>Remark.</strong> The Hahm-Banach theorem tells us that we can always extend a linear functional defined on a subspace to obtain a linear functional on the whole space that behaves in the same way. This is useful because it allows us to study the behavior of linear functionals on a larger space, which can provide more information about the structure of the original subspace.<br>From the Hahn-Banach theorem we have the following two lemmas.<br><br>\n",
    "<strong>Lemma 1.</strong>Let $U$ be a linear subspace of a normed linear space $X$ and consider $x_0 \\in X$ such that\n",
    "$$\n",
    "dist(x_0,U) \\geq \\delta,\n",
    "$$\n",
    "for some $\\delta \\gt 0$, i.e.,\n",
    "$$\n",
    "||x_0 - u|| \\geq \\delta, \\forall u \\in U.\n",
    "$$\n",
    "Then there is a bounded linear functional $L$ on $X$ such that:\n",
    "- $||L|| \\leq 1$;\n",
    "- $L(u) = 0$, $\\forall u \\in U$, i.e., $L_{|U} = 0$;\n",
    "- $L(x_0)=\\delta$.\n",
    "<br><br>\n",
    "\n",
    "<strong>Lemma 2 (reformulation of Lemma 1).</strong>Let $U$ be a linear, nondense subspace of a normed linear space $X$. Then there is a bounded linear functional $L$ on $X$ such that $L \\neq 0$ on $X$ and $L_U = 0$.<br><br>\n",
    "<strong>Lemma 3.</strong>Let $U$ be a linear, non-dense subspace of $C(I_n)$. Then there is a measure $\\mu \\in M(I_n)$ such that\n",
    "$$\n",
    "\\int_{I_n} h d\\mu = 0, \\forall h \\in U.\n",
    "$$\n",
    "Proof: Considering $X=C(I_n)$ in Lemma 2, there is a bounded linear functional $L: C(I_n) \\to \\mathbb{R}$ such that $L \\neq 0$ on $C(I_n)$ and $L_{|U} = 0$. Applying the representation theorem of linear bounded functionals on $C(I_n)$, there is a measure $\\mu \\in M(I_n)$ such that\n",
    "$$\n",
    "L(f) = \\int_{I_n} f d\\mu, \\forall f \\in C(I_n).\n",
    "$$\n",
    "In particular for any $h \\in U$, we have\n",
    "$$\n",
    "L(h) = \\int_{I_n} h d\\mu = 0.\n",
    "$$\n",
    "which is the desired result.<br><br>\n",
    "\n",
    "<strong>Remark 2.</strong> Note that $L \\neq 0$ implies $\\mu \\neq 0$.<br><br>\n",
    "<strong>Proposition.</strong> Let $\\sigma$ be ant countinuous discriminatory function. Then the finite sums of the form\n",
    "$$\n",
    "G(x)=\\sum_{j=1}^N \\alpha_j \\sigma(w_j^Tx + \\theta_j)_1 w_j \\in \\mathbb{R}^n, \\alpha_j, \\theta_j \\in \\mathbb{R}\n",
    "$$\n",
    "are dense in $C(I_n)$.\n",
    "<br>\n",
    "Proof: Since $\\sigma$ is continuous, it follows that\n",
    "$$\n",
    "U=\\left\\{G;G(x)=\\sum_{j=1}^N \\alpha_j \\sigma(w_j^Tx + \\theta_j)\\right\\}.\n",
    "$$\n",
    "is a linear subspace of $C(I_n)$. We continue the proof adopting the contradiction method.<br>\n",
    "Assume that $U$ is not dense in $C(I_n)$ i.e. we assume that the closure of $U$ is not all $C(I_n)$. Then the closure of $U$ (call it $R$) is a closed proper subspace of $C(I_n)$.<br>\n",
    "By the H-B Theorem there is a bounded linear functional on $C(I_n)$ (call it $L$) with the property that $L \\neq 0$ but $L(U)=L(R)=0$.<br>\n",
    "By the Representation Theorem $L$ is of the form\n",
    "$$\n",
    "L(h)=\\int_{I_n} h(x) d\\mu(x)\n",
    "$$\n",
    "for some $\\mu \\in M(I_n)$, for all $h \\in C(I_n)$.<br>\n",
    "In particular since $\\sigma(w^Tx+\\theta)$ is in $U$ for all $\\omega$ and $\\theta$, we must have\n",
    "$$\n",
    "\\int_{I_n} \\sigma(w^Tx+\\theta) d\\mu(x) = 0.\n",
    "$$\n",
    "However, we have assumed that $\\sigma$ was discriminatory so this implies $\\mu=0$ which contradicts our assumption; hence $S$ must be dense in $C(I_n)$.<br>\n",
    "<strong>Definition.</strong> Let\n",
    "- $P_{\\omega,\\theta}=\\left\\{x;\\omega^Tx+\\theta=0\\right\\}$ the hyperplane with normal vector $\\omega$ and intercept $\\theta$;\n",
    "- $H_{\\omega,\\theta}^+=H_{\\omega,\\theta}=\\left\\{x;\\omega^Tx+\\theta \\gt 0\\right\\}$ the positive half-space;\n",
    "- $H_{\\omega,\\theta}^-=\\left\\{x;\\omega^Tx+\\theta \\lt 0\\right\\}$ the negative half-space;\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<strong>Lemma 4.</strong> Let $\\mu \\in M(I_n)$. If $\\mu$ vanishes on all hyperplanes and open half-spaces in $\\mathbb{R}^n$, then $\\mu=0$. More precisely if\n",
    "\n",
    "$$\n",
    "\\mu(P_{\\omega,\\theta})=0, \\mu(H_{\\omega,\\theta})=0, \\forall \\omega \\in \\mathbb{R}^n, \\theta \\in \\mathbb{R},\n",
    "$$\n",
    "then $\\mu=0$.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<strong>Proposition.</strong> Any continuous sigmoidal function is discriminatory for all measures $\\mu \\in M(I_n)$.\n",
    "<br>\n",
    "Proof: Let $\\mu \\in M(I_n)$ be a fixed measure. Choose a continuous sigmoidal function that satisfies\n",
    "$$\n",
    "\\int_{I_n} \\sigma(w^Tx+\\theta) d\\mu(x) = 0, \\forall \\omega \\in \\mathbb{R}^n, \\theta \\in \\mathbb{R} \\text{(1)}.\n",
    "$$\n",
    "We need to show that $\\mu=0$. First, construct the continuous function\n",
    "$$\n",
    "\\sigma_{\\lambda}(x)=\\sigma(\\lambda(\\omega^Tx+\\theta) + \\phi)\n",
    "$$\n",
    "for given $\\omega$, $\\theta$ and $\\phi$, and use the definition of sigmoidal to note that\n",
    "$$\n",
    "\\lim_{\\lambda \\to \\infty} \\sigma_{\\lambda}(x) = \\begin{cases} 0, & \\text{if } \\omega^Tx+\\theta \\lt 0 \\\\ 1, & \\text{if } \\omega^Tx+\\theta \\gt 0 \\\\ \\sigma(\\phi), & \\text{if } \\omega^Tx + \\theta = 0 \\end{cases}\n",
    "$$\n",
    "Define the bounded function\n",
    "$$\n",
    "\\gamma(x)=\\begin{cases} 0, & \\text{if } x \\in H_{\\omega,\\theta}^- \\\\ 1, & \\text{if } x \\in H_{\\omega,\\theta}^+ \\\\ \\sigma(\\phi), & \\text{if } x \\in P_{\\omega,\\theta} \\end{cases}\n",
    "$$\n",
    "and notice that $\\sigma_{\\lambda} \\to \\gamma(x)$ pointwise on $\\mathbb{R}$, as $\\lambda \\to \\infty$. The Bounded Convergence Theorem allows switching the limit with the integral, obtaining\n",
    "$$\n",
    "\\lim_{\\lambda \\to \\infty} \\int_{I_n} \\sigma_{\\lambda}(x) d\\mu(x) = \\int_{I_n} \\gamma(x) d\\mu(x) = \\int_{H_{\\omega,\\theta}^+} \\gamma(x)d\\mu(x) + \\int_{H_{\\omega,\\theta}^-} \\gamma(x)d\\mu(x) + \\int_{P_{\\omega,\\theta}} \\gamma(x)d\\mu(x) = \\mu(H_{\\omega,\\theta}^+) + \\sigma(\\phi)\\mu(P_{\\omega,\\theta}).\n",
    "$$\n",
    "Equation (1) implies that $\\int_{I_n} \\sigma_{\\lambda}(x) d\\mu(x) = 0$, and hence the limit in previous left term vanishes. Consequently, the right term must also vanish, fact that can be written as\n",
    "$$\n",
    "\\mu(H_{\\omega,\\theta}^+) + \\sigma(\\phi)\\mu(P_{\\omega,\\theta}) = 0.\n",
    "$$\n",
    "Since this relation holds for any value of $\\phi$, taking $\\phi \\to +\\infty$ and using the properties of $\\sigma$, yields\n",
    "$$\n",
    "\\mu(H_{\\omega,\\theta}^+) + \\mu(P_{\\omega,\\theta}) = 0.\n",
    "$$\n",
    "Similarly, taking $\\phi \\to -\\infty$, implies\n",
    "$$\n",
    "\\mu(H_{\\omega,\\theta}^+) = 0, \\forall \\omega \\in \\mathbb{R}^n, \\theta \\in \\mathbb{R}. \\text{(2)}\n",
    "$$\n",
    "Note that, as a consequence of the last two relations, we also have $\\mu(P_{\\omega, \\theta})=0$. Since $H^+_{-\\omega,-\\theta}$, relation (2) states that the measure $\\mu$ vanishes on all half-spaces of $\\mathbb{R}^n$. Lemma 4 states that a measure with such properties is necessarily the zero measure, $\\mu=0$. Therefore, $\\sigma$ is discriminatory.<br>\n",
    "<strong>Proposition.</strong> The ReLU function is 1-discriminatory.\n",
    "<br>\n",
    "Proof: Let $\\mu$ be a signet Borel measure, and assume the following holds for all $y \\in \\mathbb{R}$ and $\\theta \\in \\mathbb{R}$:\n",
    "$$\n",
    "\\int ReLU(yx+ \\theta) d\\mu(x) = 0.\n",
    "$$\n",
    "We want to show that $\\mu=0$. For that, we will construct a sigmoid bounded, continuous (and therefore Borel measurable) function from subtracting two ReLU functions with different parameters. In particular, considerthe function\n",
    "$$\n",
    "f(x)=\\begin{cases} 1, & \\text{if } x \\gt 1 \\\\ 0, & \\text{if } x \\lt 0  \\\\ x, & \\text{if } 0 \\leq x \\leq 1 \\end{cases}\n",
    "$$\n",
    "Then any function of the form $g(x) = f(yx+\\theta)$ with $y \\neq 0$ can be described as\n",
    "$$\n",
    "g(x)=ReLU(yx+\\theta_1)-ReLU(yx+\\theta_2)\n",
    "$$\n",
    "by setting $\\theta_1=-\\theta/y$ and $\\theta_2=(1-\\theta)/y$. If $y=0$, then instead set\n",
    "$$\n",
    "g(x)=f(x)=\\begin{cases} ReLU(f(\\theta)), & \\text{if } f(\\theta) \\geq 0 \\\\ -ReLU(-f(\\theta)), & \\text{if } f(\\theta) \\leq 0 \\end{cases}\n",
    "$$\n",
    "Which means that for any $y \\in \\mathbb{R}$ and $\\theta \\in \\mathbb{R}$\n",
    "$$\n",
    "\\int f(yx+\\theta) d\\mu(x) = \\int (ReLU(yx+\\theta_1) - ReLU(yx+\\theta_2)) d\\mu(x) = \\int ReLU(yx+\\theta_1) d\\mu(x) - \\int ReLU(yx+\\theta_2) d\\mu(x) = 0 - 0 = 0.\n",
    "$$\n",
    "By the previous lemma, $f$ is discriminatory, and therefore, $\\mu =0$.\n",
    "<br><br>\n",
    "<strong>Definition.</strong> For $f: \\mathbb{R}^n \\to \\mathbb{R}$ an activation function we define:\n",
    "$$\n",
    "\\sum_n(f)=span\\left\\{f(y \\cdot x + \\theta)|y \\in \\mathbb{R}^n, \\theta \\in \\mathbb{R}\\right\\}.\n",
    "$$\n",
    "\n",
    "<strong>Proposition.</strong>If $\\sum_1(f)$ is dense in $C([0,1])$, then $\\sum_n(f)$ is dense in $C([0,1]^n)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
